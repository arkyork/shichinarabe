# 七並べ
Pythonで「七並べ」を実装したレポジトリとなっています。
本リポジトリでは、ランダムエージェントと強化学習エージェント（PPO）によるプレイを実現しました。

![play demo](./play_demo.gif)

[🎬 動画を再生する（play_movie.mp4）](./play_movie.mp4)

## ファイル構成

| ファイル名                | 概要                                  |
| -------------------- | ----------------------------------- |
| `normal_play.py`     | ランダムエージェント同士による七並べのプレイを実行します。       |
| `ai_play.py`         | 強化学習済みのPPOエージェントとランダムエージェントを対戦させます。 |
| `train_PPO.py`       | ランダムエージェントと対戦しながらPPOエージェントを学習します。   |
| `train_ppo_multi.py` | 自己対戦（self-play）によるさらなる強化学習を行います。    |

---

## 実行手順

1. **ランダムエージェントとの学習**

   ```bash
   python train_PPO.py
   ```

   * Stable Baselines3 の `MaskablePPO` を使用。
   * ランダムエージェントとの対戦を通じて初期学習を行います。

2. **自己対戦による学習（self-play）**

   ```bash
   python train_ppo_multi.py
   ```

   * 先に学習したモデルをもとに、エージェント同士で自己対戦を繰り返し、戦略を高度化します。

3. **学習済みモデルでプレイ**

   ```bash
   python ai_play.py
   ```

   * 学習済みのPPOエージェントを利用して七並べをプレイします。
   * `normal_play.py`では純粋なランダムエージェント同士の対戦も可能です。


## 学習の流れ

1. **ランダムエージェントとの対戦で初期学習**
   `train_PPO.py` にて `MaskablePPO` を用いて学習します。
   MaskablePPOは「行動マスク」を用いて、ルール上不可能な手を除外して学習を行います。

2. **自己対戦（self-play）で学習**
   `train_ppo_multi.py` により、学習済みエージェントを複数コピーし、自身と戦わせます。


## ゲームルール

* 各プレイヤーは順番にカードを場に出します。
* **出せるカードがない場合はパス**します。
* **4回パスしたプレイヤーは負け**となります。
* **すべての手札が最初になくなったプレイヤーが勝ち**です。





